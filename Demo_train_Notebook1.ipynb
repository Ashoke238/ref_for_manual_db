{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc68c0d-68fa-43ae-bf24-a3e5883aa041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC ## Demo_train_Notebook1\n",
    "# MAGIC This notebook is a minimal prototype for training a sample ML model on Databricks.\n",
    "# MAGIC \n",
    "# MAGIC **Purpose:** Demonstrate full notebook lifecycle for automation & job integration.\n",
    "# MAGIC \n",
    "# MAGIC - Uses mock data\n",
    "# MAGIC - Trains fast\n",
    "# MAGIC - Logs to MLflow\n",
    "# MAGIC - No hardcoding\n",
    "# MAGIC - Fully portable using config\n",
    "\n",
    "# Mock dbutils if not in Databricks environment\n",
    "# Mock dbutils if not in Databricks environment\n",
    "try:\n",
    "    dbutils\n",
    "except NameError:\n",
    "    class DBUtilsMock:\n",
    "        def notebook(self):\n",
    "            return self\n",
    "\n",
    "        def getContext(self):\n",
    "            return self\n",
    "\n",
    "        def userName(self):\n",
    "            return self\n",
    "\n",
    "        def get(self):\n",
    "            return \"mock_user@example.com\"\n",
    "\n",
    "    dbutils = DBUtilsMock()\n",
    "\n",
    "# Now proceed with the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b1bbf0-2b2c-442a-8614-5cba516857b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "%pip install pandas scikit-learn mlflow --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589e641f-24b2-4be8-bf50-ec70b51d6b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 2 - Imports & Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load runtime config from mlops_config.json\n",
    "CONFIG_PATH = \"mlops_config.json\"\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        repo_name = config.get(\"repo_name\", \"unknown-repo\")\n",
    "        branch_name = config.get(\"branch_name\", \"unknown\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"❌ Config file not found: {CONFIG_PATH}\")\n",
    "\n",
    "# Derive env from branch\n",
    "env = \"prod\" if branch_name == \"main\" else \"dev\"\n",
    "\n",
    "\n",
    "# Get current user\n",
    "try:\n",
    "    dbutils\n",
    "except NameError:\n",
    "    class DBUtilsMock:\n",
    "        def notebook(self):\n",
    "            return self\n",
    "        def getContext(self):\n",
    "            return self\n",
    "        def userName(self):\n",
    "            return self\n",
    "        def get(self):\n",
    "            return \"mock_user@example.com\"\n",
    "    dbutils = DBUtilsMock()\n",
    "\n",
    "try:\n",
    "    user_email = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "except Exception:\n",
    "    import getpass\n",
    "    user_email = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b12e8c-4763-408b-a16d-8f229055b686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 3 - Data Ingestion (Mock)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"feature1\": np.random.rand(100),\n",
    "    \"feature2\": np.random.rand(100),\n",
    "    \"label\": np.random.randint(0, 2, 100)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf16a26-acf6-4fe1-b443-01c88d5d792f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 4 - Preprocessing\n",
    "\n",
    "X = data[[\"feature1\", \"feature2\"]]\n",
    "y = data[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482551ff-c7ee-43a0-8596-1de9aef36c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# COMMAND ----------\n",
    "# Section 5.1 - Save training data to UC table (dynamic path discovery)\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Combine features + label\n",
    "    train_df_combined = X_train.copy()\n",
    "    train_df_combined[\"label\"] = y_train\n",
    "\n",
    "    # Convert to Spark DF\n",
    "    spark_train_df = spark.createDataFrame(train_df_combined)\n",
    "\n",
    "    # Step 1: Dynamically find a writable catalog + schema\n",
    "    catalogs = [row['catalog'] for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    selected_catalog = None\n",
    "    selected_schema = None\n",
    "\n",
    "    for cat in catalogs:\n",
    "        try:\n",
    "            schemas = spark.sql(f\"SHOW SCHEMAS IN {cat}\").collect()\n",
    "            for s in schemas:\n",
    "                schema_name = s['databaseName']\n",
    "                # Simple logic: pick the first one that starts with your repo name or \"default\"\n",
    "                if repo_name in schema_name or schema_name.lower() == \"default\":\n",
    "                    selected_catalog = cat\n",
    "                    selected_schema = schema_name\n",
    "                    break\n",
    "            if selected_catalog: break\n",
    "        except Exception as e:\n",
    "            continue  # some catalogs like system may error — just skip\n",
    "\n",
    "    # Step 2: Build UC path\n",
    "    if not selected_catalog or not selected_schema:\n",
    "        raise Exception(\"❌ Could not determine a valid UC catalog and schema.\")\n",
    "\n",
    "    uc_table_path = f\"{selected_catalog}.{selected_schema}.train_input_data\"\n",
    "\n",
    "    # Step 3: Save to UC Delta table\n",
    "    spark_train_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(uc_table_path)\n",
    "\n",
    "    print(f\"✅ Training data saved to Unity Catalog table: {uc_table_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Skipping UC write (non-Databricks env): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6597b2b5-1661-4519-8ba4-a075fa24eba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 5 - Train/Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cba0411-0274-40fe-8d0e-1218a7e21eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 6 - Model Training\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0129c2-6cf8-44d5-b15b-c30d88474385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 7 - Evaluation\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542da055-35f0-4f48-9318-26c0aee73448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 8 - MLflow Tracking\n",
    "\n",
    "experiment_path = f\"/Users/{user_email}/{repo_name}_train_{env}\"\n",
    "\n",
    "client = MlflowClient()\n",
    "if not client.get_experiment_by_name(experiment_path):\n",
    "    client.create_experiment(experiment_path)\n",
    "\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "run_name = f\"{repo_name}-train-{env}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.set_tags({\n",
    "        \"project\": repo_name,\n",
    "        \"notebook\": \"Demo_train_Notebook1\",\n",
    "        \"branch\": branch_name,\n",
    "        \"env\": env,\n",
    "        \"owner\": user_email,\n",
    "        \"run_type\": \"train\",\n",
    "        \"date\": datetime.today().strftime('%Y-%m-%d')\n",
    "    })\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"train_rows\", len(X_train))\n",
    "    mlflow.log_param(\"features\", X.columns.tolist())\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    print(f\"✅ Model logged to MLflow under run '{run_name}' on branch '{branch_name}' and env '{env}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a895cd5-40b8-45b4-b309-27f54e91b1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 10 - Clean Exit\n",
    "\n",
    "print(\"✅ Training complete and model logged.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo_train_Notebook1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
