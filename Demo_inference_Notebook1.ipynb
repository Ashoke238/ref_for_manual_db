{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44a3786-de99-43ca-82fd-85a26dd0d372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC ## Demo_inference_Notebook1\n",
    "# MAGIC This notebook performs inference using a previously trained and logged model from MLflow.\n",
    "# MAGIC \n",
    "# MAGIC - Uses mock input\n",
    "# MAGIC - Loads model from MLflow\n",
    "# MAGIC - Logs inference metadata\n",
    "# MAGIC - Follows staff-level modular MLOps best practices\n",
    "\n",
    "# Mock dbutils if not in Databricks environment\n",
    "try:\n",
    "    if not dbutils:\n",
    "        raise NameError\n",
    "except NameError:\n",
    "    class DBUtilsMock:\n",
    "        def notebook(self):\n",
    "            return self\n",
    "\n",
    "        def getContext(self):\n",
    "            return self\n",
    "\n",
    "        def userName(self):\n",
    "            return self\n",
    "\n",
    "        def get(self):\n",
    "            return \"mock_user@example.com\"\n",
    "\n",
    "    dbutils = DBUtilsMock()\n",
    "\n",
    "# Now proceed with the rest of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c6fe22-ee9c-4646-9db9-7071198999b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "%pip install pandas scikit-learn mlflow --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eaca37-9271-48f3-b3e0-54c36da81423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 2 - Imports & Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load config\n",
    "CONFIG_PATH = \"mlops_config.json\"\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        repo_name = config.get(\"repo_name\", \"unknown-repo\")\n",
    "        branch_name = config.get(\"branch_name\", \"unknown\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"❌ Config file not found: {CONFIG_PATH}\")\n",
    "\n",
    "# Derive env from branch\n",
    "env = \"prod\" if branch_name == \"main\" else \"dev\"\n",
    "\n",
    "# Get current user\n",
    "user_email = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b76574-e543-4fd2-aa36-78a0f76a64e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 3 - Load Model\n",
    "\n",
    "# Build experiment path\n",
    "experiment_path = f\"/Users/{user_email}/{repo_name}_train_{env}\"\n",
    "\n",
    "# Get latest run with a logged model\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_path)\n",
    "runs = client.search_runs(experiment.experiment_id, order_by=[\"start_time DESC\"], max_results=1)\n",
    "\n",
    "if not runs:\n",
    "    raise ValueError(\"❌ No MLflow runs found with logged models.\")\n",
    "\n",
    "latest_run = runs[0]\n",
    "model_uri = f\"runs:/{latest_run.info.run_id}/model\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "print(f\"✅ Loaded model from: {model_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86db91e7-2037-4fd9-9c71-b4f81232910d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 4 - Generate Mock Input\n",
    "\n",
    "# Simulate input data (2 features)\n",
    "input_data = pd.DataFrame({\n",
    "    \"feature1\": [0.15, 0.60],\n",
    "    \"feature2\": [0.75, 0.30]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a17b84e-1fa1-4cac-ae34-0279970fd1c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 5 - Inference\n",
    "\n",
    "predictions = model.predict(input_data)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa59605-cc44-4ec6-a333-3c8e3a9dcaa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 5.1 - Save predictions to Unity Catalog table (dynamically)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Step 1: Prepare prediction DataFrame\n",
    "pred_df = pd.DataFrame({\"prediction\": predictions})\n",
    "spark_pred_df = spark.createDataFrame(pred_df)\n",
    "\n",
    "# Step 2: Dynamically find a UC catalog + schema\n",
    "catalogs = [row['catalog'] for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "selected_catalog = None\n",
    "selected_schema = None\n",
    "\n",
    "for cat in catalogs:\n",
    "    try:\n",
    "        schemas = spark.sql(f\"SHOW SCHEMAS IN {cat}\").collect()\n",
    "        for s in schemas:\n",
    "            schema_name = s['databaseName']\n",
    "            if repo_name in schema_name or schema_name.lower() == \"default\":\n",
    "                selected_catalog = cat\n",
    "                selected_schema = schema_name\n",
    "                break\n",
    "        if selected_catalog: break\n",
    "    except Exception:\n",
    "        continue  # skip non-accessible/system catalogs\n",
    "\n",
    "# Step 3: Build full UC table path\n",
    "if not selected_catalog or not selected_schema:\n",
    "    raise Exception(\"❌ Could not find a valid UC catalog/schema.\")\n",
    "\n",
    "uc_output_table = f\"{selected_catalog}.{selected_schema}.inference_output\"\n",
    "\n",
    "# Step 4: Save predictions as Delta table in UC\n",
    "spark_pred_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(uc_output_table)\n",
    "\n",
    "print(f\"✅ Inference predictions saved to Unity Catalog table: {uc_output_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89576324-3709-4978-a3a3-9797d0ac5a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 6 - MLflow Tracking (Inference)\n",
    "\n",
    "inference_exp_path = f\"/Users/{user_email}/{repo_name}_inference_{env}\"\n",
    "\n",
    "# Ensure experiment exists\n",
    "if not client.get_experiment_by_name(inference_exp_path):\n",
    "    client.create_experiment(inference_exp_path)\n",
    "\n",
    "mlflow.set_experiment(inference_exp_path)\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "run_name = f\"{repo_name}-inference-{env}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.set_tags({\n",
    "        \"project\": repo_name,\n",
    "        \"notebook\": \"Demo_inference_Notebook1\",\n",
    "        \"branch\": branch_name,\n",
    "        \"env\": env,\n",
    "        \"owner\": user_email,\n",
    "        \"run_type\": \"inference\",\n",
    "        \"source_model_uri\": model_uri,\n",
    "        \"date\": datetime.today().strftime('%Y-%m-%d')\n",
    "    })\n",
    "\n",
    "    mlflow.log_param(\"input_rows\", len(input_data))\n",
    "    mlflow.log_param(\"model_uri\", model_uri)\n",
    "\n",
    "    # Optional: log predictions as artifact or sample\n",
    "    prediction_sample = model.predict(input_data)\n",
    "    prediction_df = pd.DataFrame(prediction_sample)\n",
    "    temp_path = \"/tmp/inference_output.csv\"\n",
    "    prediction_df.to_csv(temp_path, index=False)\n",
    "    mlflow.log_artifact(temp_path)\n",
    "\n",
    "\n",
    "    print(f\"✅ Inference logged to MLflow run '{run_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42bc799-7d1c-4fa8-a4de-6b6c4ea84d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Section 10 - Clean Exit\n",
    "\n",
    "print(\"✅ Inference completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo_inference_Notebook1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
